---
title: "N-gram model based word prediction for smart keyboard"
author: "Sridhar Pilli"
date: "April 29, 2016"
output: html_document
---

# Summary
N-gram probabilities are used to predict the next word given a sequence of words in a sentence. In this paper, first an exploratory data analysis is carried out on the text data obtained from various online web sources - news, blogs and twitter. Second N-gram probabilities are calculated and associated models are built to predict the next word given a sequence of words. Finally the models are compared and the model with least perplexity is chosen. The model built out of this project would ultimately be to use in a shiny app that simulates a predictive keyboard. 

# Exploratory data analysis
In this section exploratory data analysis is carried out to undersand word, bigram and trigram frequencies.

## Data set
Lets first figure out how big the data set is. 
```{r,echo=FALSE}
setwd("~/Documents/Git/Github/Word_Predictor/")
```
```{r,warning=FALSE,message=FALSE}
# Read number of lines,words in each file and measure its size
system("wc -lwc final/en_US/*.txt > temp.txt")
a <- read.table("temp.txt"); 
a <- data.frame("Filename" = a$V4, "Lines" = prettyNum(a$V1,big.mark = ","),
                "Words" = prettyNum(a$V2,big.mark = ","), "Size.MB" = round(a$V3/2^20,2))
library(knitr); kable(a,align="c")
```

There are about 4 million lines of text containing about 100 million words spread in files of size 550MB. Lets read a portion of this data into workspace and carry out analysis on that.


## Training, Validation and Test Data sets
Lets split 60% of data into training, 20% of data into validation and the remaning 20% of data into testing data sets. The idea is to grab a set of lines from each of the sources of data - blogs, news and twitter to for a particular type of data set - training/validation/testing sets.

```{r,warning=FALSE,message=FALSE}
# Process each input file. 
Nlines <- 200000
d.b <- readLines("final/en_US/en_US.blogs.txt",Nlines)
d.n <- readLines("final/en_US/en_US.news.txt",Nlines)
d.t <- readLines("final/en_US/en_US.twitter.txt",Nlines)

# Split data into training, validation and test sets
set.seed(100)
idx <- sample(seq(Nlines)); N1 <- round(0.6*Nlines); N2 <- round(0.8*Nlines)
train <- c(d.b[idx[1:N1]],          d.n[idx[1:N1]],          d.t[idx[1:N1]])
valid <- c(d.b[idx[(N1+1):N2]],     d.n[idx[(N1+1):N2]],     d.t[idx[(N1+1):N2]])
test  <- c(d.b[idx[(N2+1):Nlines]], d.n[idx[(N2+1):Nlines]], d.t[idx[(N2+1):Nlines]])
```
```{r,echo=FALSE}
x <- data.frame(c("Training Set","Validation Set","Testing Set", "Total"),
                prettyNum(3*c(N1,N2-N1,Nlines-N2,Nlines),big.mark = ","),
                round(c(N1,N2-N1,Nlines-N2,Nlines)/Nlines*100,1) )
names(x) <- c("Data Set", "Num lines", "Percentage lines")
library(knitr);kable(x,align = "c")
```

Lets now sample the training set and carry out analysis on uni-gram, bi-gram and tri-grams. Since modeling these N-grams is a computationally expensive step, I would like to take the approach of sampling the training set - obtain some metrics - infer the statistics of the  population (ie., entire training set). 

## Data cleansing and pre-processing
The following steps are carried out as part of **ngram_tokenize()** function while carrying out the analysis. This is included as part of **ngram_compute()** function. Refer to appendix section for implementation details.

1. Split the raw text sentences into words
2. Filter out punctuations. 
3. Convert all words to lowercase.
4. Filter out profane words. List of profane words are taken from [_**CMU website**_](http://www.cs.cmu.edu/~biglou/resources/bad-words.txt)
5. Filter out stop words. List of stop word taken from [_**MIT Paper**_](http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-list/english.stop)

```{r,echo=FALSE}
fileurl <- "http://www.cs.cmu.edu/~biglou/resources/bad-words.txt"
if(!file.exists("bad-words.txt")) { download.file(fileurl,destfile = "bad-words.txt",method="curl") }

fileurl <- "http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-list/english.stop"
if(!file.exists("stop-words.txt")) { download.file(fileurl,destfile = "stop-words.txt", method="curl") }

ngram_tokenize <- function(data) {
    data <- lapply(data, function(x) { tolower(gsub("[[:punct:]]","", x)) }) # punctuation, lowercase
    data <- lapply(data, function(x) unlist(strsplit(x,split=" ")) ) # into words
    data <- lapply(data, function(x) { gsub("[0-9]","",x)}) # remove numbers
    data <- lapply(data, function(x) x[!(x=="")]) # remove blanks

    # Apply profanity filter
    profanity <- readLines("bad-words.txt"); 
    data <- lapply(data, function(x) { x[!(x %in% profanity[-1])] })
}
```

## Uni-gram analysis
Lets look at the unigrams and their frequency of occurence in the training sets. The way to go about is to sample a small subset of data from the training set - compute the unigrams and their counts - repeat it for many such small subsets - average across all sample sets. To start with a sample size of **100** lines of text and **100** such samples are chosen. This equates to **10,000** lines of text.

```{r,echo=FALSE}
ngram_compute <- function(data,n,dictionary=character(0)) {
    L <- length(data)
    data <- ngram_tokenize(data) # Tokenize the data
    
    # Create n-grams
    if(n==2) { # Bigrams
        data <- sapply(data,function(x) x[x %in% dictionary]) # select words in dictionary
        idx2 <- sapply(data, function(x) ifelse(length(x)>1,1,0)) # rows with atleast 2 words
        data <- lapply(data[as.logical(idx2)],function(x) {
            LX <- length(x)
            mapply(function(y,z) { paste(y,z,collapse = " ") },x[1:(LX-1)],x[2:LX], USE.NAMES = FALSE)
        })
        L <- length(data)
    }
    if(n==3) { # Trigrams
        data <- sapply(data,function(x) x[x %in% dictionary]) # select words in dictionary
        idx3 <- sapply(data, function(x) ifelse(length(x)>2,1,0)) # rows with atleast 3 words
        data <- lapply(data[as.logical(idx3)],function(x) {
            LX <- length(x)
            mapply(function(p,q,r) {paste(p,q,r,collapse = " ")},x[1:(LX-2)],x[2:(LX-1)],x[3:LX], USE.NAMES = FALSE)
        })
        L <- length(data)
    }
    
    # Count unique n-grams
    unique_ngrams <- unique(unlist(data))
    ngram <- matrix(0,length(unique_ngrams),L); rownames(ngram) <- unique_ngrams; colnames(ngram) <- 1:L
    for(i in 1:L) { 
        ns <- table(data[[i]])
        ngram[names(ns),i] <- ns 
    }
    
    ngram
}
```

```{r}
M <- 10000
# Compute unigrams
unigrams <- ngram_compute(train[1:M],1) # Refer appendix for function definition

# Compute unigrams without stop words
stopWords <- readLines("stop-words.txt"); stopWords <- gsub("[[:punct:]]","",stopWords)
unigrams_without_stopwords <- unigrams[!(rownames(unigrams) %in% stopWords),]
```

### Statistical Inference
Using the central limit theorem to infer the population statistics from sample statistics, the average counts from the samples estimates to the counts seen in population. Note the counts are not integers as they are averaged over all sample sets and hence they are estimates. 

**Top 10 unigrams with stop words and their average counts are**
```{r,warning=FALSE,message=FALSE}
um1_all <- unigrams %*% matrix(1,M,1) #row sums
names(um1_all) <- rownames(unigrams); um1_all <- sort(um1_all, decreasing = TRUE)
um1_all[1:10]
library(wordcloud)
wordcloud(names(um1_all),um1_all,max.words=350,scale = c(3,0.5),rot.per = 0.2, colors = brewer.pal(6,"Dark2"),random.order = FALSE)
```

**The top 10 unigrams without stop words and their average counts are**
```{r,warning=FALSE,message=FALSE}
um2_all <- unigrams_without_stopwords %*% matrix(1,M,1) # row sums 
names(um2_all) <- rownames(unigrams_without_stopwords); um2_all <- sort(um2_all,decreasing = TRUE)
um2_all[1:10]
wordcloud(names(um2_all),um2_all,max.words=150, scale = c(3,0.5),rot.per = 0.2, colors = brewer.pal(6,"Dark2"),random.order = FALSE)
```


```{r,fig.width=10,fig.height=6,fig.align="center"}
mask_mat <- matrix(0,M,100); for(i in 1:100) mask_mat[(((i-1)*100+1):(i*100)),i] <- 1
ums1 <- unigrams[names(um1_all[1:10]),] %*% mask_mat; rownames(ums1) <- names(um1_all[1:10])
ums2 <- unigrams_without_stopwords[names(um2_all[1:10]),] %*% mask_mat; rownames(ums2) <- names(um2_all[1:10])

par(mfrow=c(1,2),mar=c(5,5,2,2))
boxplot(t(ums1[10:1,]),xlab = "frequency (Number of unigrams per 100 lines)", 
        main = "Unigram frequencies",horizontal = T,las=1)
boxplot(t(ums2[10:1,]),xlab = "frequency (Number of unigrams per 100 lines)", 
        main = "Unigram frequencies without stop words",horizontal = T,las=1)
```
The stop words have very high frequency as they occur often and so they tend to mislead the relevant unigrams in the language. In the plot above distributions with and without the stop words are shown. Overall there are **30,834** unigrams. The top 10 contenders without the stop words are **time, people, day, good, back, year, make, love, years, work**. 

### Dictionary selection
```{r}
round(c(sum(um1_all[1:110]), sum(um1_all[1:7000]))/sum(um1_all)*100,2)
```

In the test sample set considered with sample size of **100** lines and **100** such samples, there are **10,000** lines of text and **400,663** words. Data shows that in a frequency sorted word dictionary, only **110** unique words are neeed to cover about **50%** of all the word instances. Likewise only **7,000** unique words (15% of vocabulary) are needed to cover about **90%** of all word instances. Lets select the top **7,000** words (inclusive of stop words) to be in the dictionary to get 90% coverage.
```{r}
dictionary <- names(um1_all[1:7000])
um1 <- um1_all[1:7000]

sum(um2_all[1:15000])/sum(um2_all)
um2 <- um2_all[1:15000]
```
```{r,echo=FALSE}
rm(unigrams,ums1,ums2)
```
Likewise when stop words are discarded we can get 90% coverage with about **15,000** words

## Bi-gram and Tri-gram analysis
Lets look at the bigrams as well as trigrams and their frequency of occurence in the training sets. Here words that are not present in the dictionary are filtered out, however stop words are not filtered out. In my exploration I noticed this is the most important step to have **all** bigrams or trigrams generated from a selected dictionary while building the model. 

```{r,eval=FALSE}
# Compute bigrams and trigrams
bigrams <- ngram_compute(train[1:M],2,dictionary)
trigrams <- ngram_compute(train[1:M],3,dictionary)

bm <- bigrams %*% matrix(1,ncol(bigrams),1); names(bm) <- rownames(bigrams); bm <- sort(bm,decreasing = TRUE)
tm <- trigrams %*% matrix(1,ncol(trigrams),1); names(tm) <- rownames(trigrams); tm <- sort(tm,decreasing = TRUE)

mask_mat <- matrix(0,9000,100); for(i in 1:90) mask_mat[(((i-1)*100+1):(i*100)),i] <- 1
bms <- bigrams[names(bm[1:10]),1:9000] %*% mask_mat; rownames(bms) <- names(bm[1:10])
tms <- trigrams[names(tm[1:10]),1:9000] %*% mask_mat; rownames(tms) <- names(tm[1:10])
```
```{r,echo=FALSE}
load("Report.RData")
```

**The top 10 bigrams and their total counts are**
```{r,echo=FALSE,eval=TRUE,warning=FALSE,message=FALSE}
bm[1:10]
wordcloud(names(bm),bm,max.words=150,scale=c(3,0.5),rot.per = 0.2, colors = brewer.pal(6,"Dark2"),random.order = FALSE)
```

**The top 10 trigrams and their total counts are**
```{r,echo=FALSE,eval=TRUE,warning=FALSE,message=FALSE}
tm[1:10]
wordcloud(names(tm),tm,max.words=100,scale=c(3,0.1),rot.per = 0.3, colors = brewer.pal(6,"Dark2"),random.order = FALSE)
```

There are **148,696** bigrams and **290,922** trigrams identified and their counts calculated.

```{r,fig.width=10,fig.height=6,fig.align="center"}
par(mfrow = c(1,2), mar=c(4,6,2,2))
boxplot(t(bms[10:1,]),xlab = "frequency (of 100 lines)", main = "Bigram frequencies",horizontal = T,las=1)
boxplot(t(tms[10:1,]),xlab = "frequency (of 100 lines)", main = "Trigram frequencies",horizontal = T,las=1)
```


# Prediction model
In this section various models would be computed and evaluated to select the best model. 

## Model Building 
Unigram, Bigram and Trigram models built on sampled sets are accummulated to simulate a giant 

1. **Smoothing** : Laplace smoothing of adding 1 to all the counts is used here to calculate probabilities for those missing unigrams. Out of vocaboulary words are marked as "UNK"
2. **Backoff** : Stupid backoff method to the next (n-1)gram approach has been considered to look for bigrams when trigrams do not exist in the model and look for unigrams when bigrams do not exist in the model. 

### Unigram, Bigram and Trigram probabilities
Compute unigram, bigram and trigram probability tables in this step. They are used to evaluate models and also predict the next word. 
```{r}
# Calculate unigram log likelihood
N <- sum(um1) # Total number of tokens
V <- length(um1) # Number of words in the dictionary
count <- c(um1, "UNK" = 0) + 1 # Laplacian smoothing
MLE <- log( count / (N+V) )
um1.mat <- as.matrix(cbind(count,MLE))

N2 <-sum(um2); V2 <- length(um2);
count <- c(um2, "UNK" = 0) + 1; MLE <- log( count / (N2+V2) )
um2.mat <- as.matrix(cbind(count,MLE)) # without stop words

# Calculate bigram log likelihood
PQ <- names(bm)
P <- sapply(strsplit(PQ, split = " "), function(x) x[1])
count <- (bm + 1); MLE <- log(count/um1.mat[P,1])
bm.mat.1 <- as.matrix(cbind(count,MLE)) # "Word1" "Word2"
bm.mat.2 <- as.matrix(cbind(rep(1,V),log(1/um1.mat[1:V,1]))) # "Word1" "UNK"
rownames(bm.mat.2) <- sapply(names(um1),function(x) paste(x,"UNK",collapse = " "),USE.NAMES = FALSE)
bm.mat.3 <- as.matrix(cbind(1,um1.mat["UNK",2])); rownames(bm.mat.3) <- "UNK UNK" # "UNK" "UNK"
bm.mat <- rbind(bm.mat.1,bm.mat.2,bm.mat.3)

# Calculate trigram log likelihood
PQR <- names(tm)
PQ <- sapply(strsplit(PQR, split = " "), function(x) paste(x[1:2],collapse = " "))
count <- (tm+1); MLE <- log(count/bm.mat[PQ,1])
tm.mat <- as.matrix(cbind(count,MLE))

# Save models to disk
save(um1.mat,um2.mat,bm.mat,tm.mat, file="Model.RData")
```

The number of tokens in the model **N = 365,015** and the number of words in the dictionary **V = 7,000**.

**Top 10 unigrams, bigrams and trigrams from the corpus are listed below**
```{r,echo=FALSE,eval=TRUE}
x <- data.frame(rownames(um1.mat[1:10,]),um1.mat[1:10,1],round(um1.mat[1:10,2],1), 
                rownames(um2.mat[1:10,]),um2.mat[1:10,1],round(um2.mat[1:10,2],1),
                rownames(bm.mat[1:10,]),bm.mat[1:10,1],round(bm.mat[1:10,2],1),
                rownames(tm.mat[1:10,]),tm.mat[1:10,1],round(tm.mat[1:10,2],1),
                row.names = NULL)
names(x) = c("Unigrams","Count","MLE","Unigrams(no stop)","Count","MLE","Bigrams","Count","MLE",
             "Trigrams","Count","MLE")
kable(x,align="c")
```

**A cursory check** : The log probability values calculated fall in between -inf to 0 that maps to probability values of 0 to 1 as expected.
```{r,fig.width=10}
summary(um1.mat[,2]); summary(um2.mat[,2]); summary(bm.mat[,2]); summary(tm.mat[,2])
par(mfrow=c(1,4))
hist(um1.mat[,2],xlab="MLE", main= "Unigrams", col = "blue"); hist(um2.mat[,2],xlab = "MLE", main="Unigrams(no stop)")
hist(bm.mat[,2],xlab="MLE", main="Bigrams", col = "green")
hist(tm.mat[,2],xlab="MLE", main="Trigrams",col = "salmon")
```

## Model comparison and selection
Lets compare the three models individually and as a group to see how best they work on the validation set. Perplexity would be calculated on the validation set and the one with the least perplexity wins.
```{r,echo=FALSE}
bigram_mle <- function(D,um1.mat,bm.mat,flag=0) {
    D[!(D %in% rownames(um1.mat))] <- "UNK"; L <- length(D)
    if(flag == 0) { 
        Y <- D[1:(L-1)]
        Z <- D[2:L]
    } else { # special case for predicted word
        Z <- rownames(um1.mat); Z <- Z[1:(length(Z)-1)] # Map against dictionary  
        Y <- rep(D[L],length(Z))
    }
    YZ <- mapply(function(y,z) paste(y,z,collapse = " "), Y, Z, USE.NAMES = F)
    id1 <- (YZ %in% rownames(bm.mat))
    id2 <- (!id1) & (Y != "UNK")
    id3 <- (!id1) & (Y == "UNK")
    a <- matrix(NA,length(YZ),1)
    if(sum(id1)>0) { a[id1] <- bm.mat[YZ[id1],2] }
    if(sum(id2)>0) { a[id2] <- log(1/um1.mat[Y[id2],1]) }
    if(sum(id3)>0) { a[id3] <- um1.mat["UNK",2] }
    rownames(a) <- YZ; colnames(a) <- "MLE"
    a
}
trigram_mle <- function(D,um1.mat,bm.mat,tm.mat,flag = 0) {
    D[!(D %in% rownames(um1.mat))] <- "UNK"; L <- length(D)
    if(flag == 0) {
        P <- D[1:(L-2)]; Q <- D[2:(L-1)]; R <- D[3:L]
    }else { # special case for predicted word
        R <- rownames(um1.mat); R <- R[1:(length(R)-1)] # Map against dictionary
        P <- rep(D[L-2],length(R)) 
        Q <- rep(D[L-1],length(R))
    }
    PQR <- mapply(function(p,q,r) paste(p,q,r,collapse = " "),P,Q,R,USE.NAMES = F)
    id1 <- (PQR %in% rownames(tm.mat))
    PQ <- mapply(function(p,q) paste(p,q,collapse = " "),P,Q,USE.NAMES = F)
    id2 <- (!id1) & (PQ %in% rownames(bm.mat))
    id3 <- (!id1) & (!(PQ %in% rownames(bm.mat)))
    
    a <- matrix(NA,length(PQR),1)
    if(sum(id1)>0) { a[id1] <- tm.mat[PQR[id1],2] }
    if(sum(id2)>0) { a[id2] <- log(1/bm.mat[PQ[id2],1]) }
    if(sum(id3)>0) { a[id3] <- um1.mat["UNK",2] }
    
    rownames(a) <- PQR; colnames(a) <- "MLE"
    a
}
ngram_perplexity <- function(data,um1.mat,um2.mat,bm.mat,tm.mat,n) {
    D <- ngram_tokenize(data); D <- unlist(D); 
    if(n==1) { # Unigram
        D[!(D %in% rownames(um1.mat))] <- "UNK"
        a <- um1.mat[D,2]
        n_perp <- exp(-sum(a)/length(a))
    }
    if(n==2) { # Unigram without stop words
        D[!(D %in% rownames(um2.mat))] <- "UNK"
        a <- um2.mat[D,2]
        n_perp <- exp(-sum(a)/length(a))
    }
    if(n==3) { # Bigram model
        a <- bigram_mle(D,um1.mat,bm.mat)
        n_perp <- exp(-sum(a)/length(a))
    }
    if(n==4) { # Trigram model
        a <- trigram_mle(D,um1.mat,bm.mat,tm.mat)
        n_perp <- exp(-sum(a)/length(a))
    }
    n_perp
}
```

```{r}
u1_perp <- ngram_perplexity(valid[1:1000],um1.mat,um2.mat,bm.mat,tm.mat,1) # Unigram model perplexity
u2_perp <- ngram_perplexity(valid[1:1000],um1.mat,um2.mat,bm.mat,tm.mat,2) # Unigram without stop words
b_perp <- ngram_perplexity(valid[1:1000],um1.mat,um2.mat,bm.mat,tm.mat,3) # Bigram model perplexity
t_perp <- ngram_perplexity(valid[1:1000],um1.mat,um2.mat,bm.mat,tm.mat,4) # Trigram model perplexity
```
```{r,echo=FALSE}
x <- data.frame(Model = c("Unigram","Unigram (no stop words)","Bigram","Trigram"),
                Num = prettyNum(c(nrow(um1.mat), nrow(um2.mat), nrow(bm.mat), nrow(tm.mat)),big.mark = ","),
                Perp = round(c(u1_perp, u2_perp, b_perp, t_perp),2))
names(x) <- c("Model Type", "Number of unique n-grams", "Perplexity (validation set)")
kable(x,align="c")
```

The idea here is to use limited vocabulary unigram, bigram and trigram dictionaries and see what best we can do. While calculating perplexities the approach considered is to use log likelihoods from the model if the ngrams exist in the vocabulary, else fall back onto next (n-1)gram and trickle down to unigrams if nothing works.

From the perplexity numbers, the model of choice would be **bigram model - with smoothing** as it has the least perplexity on validation data set.

## Example prediction
Lets run through example test cases
```{r,echo=FALSE}
ngram_predict <- function(data) {
    load("Model.RData")
    
    D <- ngram_tokenize(data) # Tokenize
    D <- unlist(D); D[!(D %in% rownames(um1.mat))] <- "UNK" # mark the ones not in dictionary as UNK
    L <- length(D)
    
    # Unigram model
    um.mle <- sum(um1.mat[D,2]) + um1.mat[1,2] # going to predict always the most frequent unigram
    um.word <- rownames(um1.mat)[1]
    
    # Bigram model
    b.stat <- bigram_mle(D,um1.mat,bm.mat)
    b.dict <- bigram_mle(D,um1.mat,bm.mat,1)
    bm.mle <- sum(b.stat) + b.dict
    bm.word <- rownames(um1.mat)[which.max(bm.mle)]
    
    # Trigram model
    t.stat <- trigram_mle(D,um1.mat,bm.mat,tm.mat)
    t.dict <- trigram_mle(D,um1.mat,bm.mat,tm.mat,1)
    tm.mle <- sum(t.stat) + t.dict
    tm.word <- rownames(um1.mat)[which.max(tm.mle)]
    
    ifelse(max(bm.mle)>um.mle, bm.word, um.word)
    #a <- data.frame(predict = c(um.word,bm.word,tm.word), mle = c(um.mle,max(bm.mle),max(tm.mle)))
}
```

```{r,eval=TRUE}
ngram_predict("This looks")
ngram_predict("This looks like")
ngram_predict("This looks like a")
ngram_predict("This looks like a lot")
ngram_predict("of course thank")
ngram_predict("see you next")
```

It appers that the model is more tuned towards predicting stop words as they are more frequent. Next step would be to tweak the model to predict better.

# Appendix - Code

### ngram_tokenize()
Function that tokenizes lines of text into words
```{r,eval=FALSE}
ngram_tokenize <- function(data) {
    data <- lapply(data, function(x) { tolower(gsub("[[:punct:]]","", x)) }) # punctuation, lowercase
    data <- lapply(data, function(x) unlist(strsplit(x,split=" ")) ) # into words
    data <- lapply(data, function(x) { gsub("[0-9]","",x)}) # remove numbers
    data <- lapply(data, function(x) x[!(x=="")]) # remove blanks

    # Apply profanity filter
    profanity <- readLines("bad-words.txt"); 
    data <- lapply(data, function(x) { x[!(x %in% profanity[-1])] })
}
```


### ngram_compute()
Function to compute counts of ngrams. This is equivalent to NGramTokenizer() and TermDocumentMatrix() combined.
```{r, eval = FALSE}
ngram_compute <- function(data,n,dictionary=character(0)) {
    L <- length(data)
    data <- ngram_tokenize(data) # Tokenize the data
    
    # Create n-grams
    if(n==2) { # Bigrams
        data <- sapply(data,function(x) x[x %in% dictionary]) # select words in dictionary
        idx2 <- sapply(data, function(x) ifelse(length(x)>1,1,0)) # rows with atleast 2 words
        data <- lapply(data[as.logical(idx2)],function(x) {
            LX <- length(x)
            mapply(function(y,z) { paste(y,z,collapse = " ") },x[1:(LX-1)],x[2:LX], USE.NAMES = FALSE)
        })
        L <- length(data)
    }
    if(n==3) { # Trigrams
        data <- sapply(data,function(x) x[x %in% dictionary]) # select words in dictionary
        idx3 <- sapply(data, function(x) ifelse(length(x)>2,1,0)) # rows with atleast 3 words
        data <- lapply(data[as.logical(idx3)],function(x) {
            LX <- length(x)
            mapply(function(p,q,r) {paste(p,q,r,collapse = " ")},x[1:(LX-2)],x[2:(LX-1)],x[3:LX], USE.NAMES = FALSE)
        })
        L <- length(data)
    }
    
    # Count unique n-grams
    unique_ngrams <- unique(unlist(data))
    ngram <- matrix(0,length(unique_ngrams),L); rownames(ngram) <- unique_ngrams; colnames(ngram) <- 1:L
    for(i in 1:L) { 
        ns <- table(data[[i]])
        ngram[names(ns),i] <- ns 
    }
    
    ngram
}
```

### ngram_perplexity()
Function to compute perplexity of a test data set. Loops are unrolled for fast performance
```{r,eval=FALSE}
ngram_perplexity <- function(D,um1.mat,um2.mat,bm.mat,tm.mat,n) {
    D <- ngram_tokenize(D); D <- unlist(D); 
    if(n==1) { # Unigram
        D[!(D %in% rownames(um1.mat))] <- "UNK"
        a <- um1.mat[D,2]
        n_perp <- exp(-sum(a)/length(a))
    }
    if(n==2) { # Unigram without stop words
        D[!(D %in% rownames(um2.mat))] <- "UNK"
        a <- um2.mat[D,2]
        n_perp <- exp(-sum(a)/length(a))
    }
    if(n==3) { # Bigram model
        D[!(D %in% rownames(um1.mat))] <- "UNK"; L <- length(D)
        YZ <- mapply(function(y,z) paste(y,z,collapse = " "), D[1:(L-1)], D[2:L], USE.NAMES = F)
        Y <- D[1:(L-1)]
        id1 <- (YZ %in% rownames(bm.mat))
        id2 <- (!id1) & (Y != "UNK")
        id3 <- (!id1) & (Y == "UNK")
        a <- matrix(NA,L-1,1)
        a[id1] <- bm.mat[YZ[id1],2]; 
        a[id2] <- bm.mat[sapply(Y[id2],function(x) paste(x,"UNK",collapse = " "),USE.NAMES = FALSE),2]
        a[id3] <- bm.mat["UNK UNK",2]
    
        names(a) <- YZ
        n_perp <- exp(-sum(a)/length(a))
    }
    if(n==4) { # Trigram model with backoff
        D[!(D %in% rownames(um1.mat))] <- "UNK"; L <- length(D)
        PQR <- mapply(function(p,q,r) paste(p,q,r,collapse = " "), D[1:(L-2)],D[2:(L-1)], D[3:L], USE.NAMES = F)
        id1 <- (PQR %in% rownames(tm.mat))
        PQ <- mapply(function(p,q) paste(p,q,collapse = " "), D[1:(L-2)], D[2:(L-1)], USE.NAMES = F)
        id2 <- (!id1) & (PQ %in% rownames(bm.mat))
        id3 <- (!id1) & (!(PQ %in% rownames(bm.mat)))

        a <- matrix(NA,L-2,1)
        a[id1] <- tm.mat[PQR[id1],2]
        a[id2] <- log(1/bm.mat[PQ[id2],1])
        a[id3] <- um1.mat["UNK",2]
        
        names(a) <- PQR
        n_perp <- exp(-sum(a)/length(a))
    }
    n_perp
}
```

### ngram_predict()
Function to predict the next word given a sequence of words
```{r,eval=FALSE}
ngram_predict <- function(D) {
    load("Model.RData")
    
    D <- ngram_tokenize(D) # Tokenize
    D <- unlist(D); D[!(D %in% rownames(um1.mat))] <- "UNK" # mark the ones not in dictionary as UNK
    L <- length(D)
    
    # Unigram model
    um.mle <- sum(um1.mat[D,2]) + um1.mat[1,2] # going to predict always the most frequent unigram
    um.word <- rownames(um1.mat)[1]
    
    # Bigram model
    YZ <- mapply(function(y,z) paste(y,z,collapse = " "), D[1:(L-1)], D[2:L], USE.NAMES = F)
    Y <- D[1:(L-1)]
    id1 <- (YZ %in% rownames(bm.mat))
    id2 <- (!id1) & (Y != "UNK")
    id3 <- (!id1) & (Y == "UNK")
    b.stat <- matrix(NA,L-1,1)
    b.stat[id1] <- bm.mat[YZ[id1],2]; 
    b.stat[id2] <- bm.mat[sapply(Y[id2],function(x) paste(x,"UNK",collapse = " "),USE.NAMES = FALSE),2]
    b.stat[id3] <- bm.mat["UNK UNK",2]
    b.stat <- sum(b.stat)
    
    P <- D[L]
    Q <- rownames(um1.mat) # Map against dictionary  
    PQ <- mapply(function(p,q) paste(p,q,collapse = " "), P, Q, USE.NAMES = F)
    id1 <- (PQ %in% rownames(bm.mat))
    id2 <- (!id1) & (P != "UNK")
    id3 <- (!id1) & (P == "UNK")
    bm.mle <- matrix(NA,L-1,1)
    bm.mle[id1] <- bm.mat[PQ[id1],2]; 
    bm.mle[id2] <- bm.mat[paste(P,"UNK",collapse = " "),2]
    bm.mle[id3] <- bm.mat["UNK UNK",2]
    
    bm.mle <- bm.mle + b.stat
    bm.word <- rownames(um1.mat)[which.max(bm.mle)]
    bm.mle <- b.stat + max(bm.mle)
    
    # Trigram model
    PQR <- mapply(function(p,q,r) paste(p,q,r,collapse = " "), D[1:(L-2)],D[2:(L-1)], D[3:L], USE.NAMES = F)
    id1 <- (PQR %in% rownames(tm.mat))
    PQ <- mapply(function(p,q) paste(p,q,collapse = " "), D[1:(L-2)], D[2:(L-1)], USE.NAMES = F)
    id2 <- (!id1) & (PQ %in% rownames(bm.mat))
    id3 <- (!id1) & (!(PQ %in% rownames(bm.mat)))
    t.stat <- matrix(NA,L-2,1)
    t.stat[id1] <- tm.mat[PQR[id1],2]
    t.stat[id2] <- log(1/bm.mat[PQ[id2],1])
    t.stat[id3] <- um1.mat["UNK",2]
    t.stat <- sum(t.stat)
    
    AB <- PQ[L-2]
    C <- rownames(um1.mat) # Map against dictionary  
    ABC <- mapply(function(ab,c) paste(ab,c,collapse = " "), AB, C, USE.NAMES = F)
    id1 <- (ABC %in% rownames(tm.mat))
    id2 <- (!id1) & (AB %in% rownames(bm.mat))
    id3 <- (!id1) & (!(AB %in% rownames(bm.mat)))
    tm.mle <- matrix(NA,L-2,1)
    tm.mle[id1] <- tm.mat[ABC[id1],2]
    tm.mle[id2] <- log(1/bm.mat[AB[id2],1])
    tm.mle[id3] <- um1.mat["UNK",2]

    tm.mle <- tm.mle + t.stat
    tm.word <- rownames(um1.mat)[which.max(tm.mle)]
    tm.mle <- t.stat + max(tm.mle)
    
    # return
    retval <- data.frame(model = c("unigram","bigram","trigram"),
                         mle = c(um.mle,bm.mle,tm.mle),
                         word = c(um.word,bm.word,tm.word))
}
```

### Others
Code to download files
```{r,eval=FALSE}
fileurl <- "http://www.cs.cmu.edu/~biglou/resources/bad-words.txt"
if(!file.exists("bad-words.txt")) { download.file(fileurl,destfile = "bad-words.txt",method="curl") }

fileurl <- "http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-list/english.stop"
if(!file.exists("stop-words.txt")) { download.file(fileurl,destfile = "stop-words.txt", method="curl") }
```
