---
title: "N-gram model based word prediction for smart keyboard"
author: "Sridhar Pilli"
output: html_document
---

# Summary
N-gram probabilities are used to predict the next word given a sequence of words in a sentence. In this paper, first an exploratory data analysis is carried out on the text data obtained from various online web sources - news, blogs and twitter. Second N-gram probabilities are calculated and associated models are built to predict the next word given a sequence of words. Finally the models are compared and the model with least perplexity is chosen. The model built out of this project would ultimately be to use in a shiny app that simulates a predictive keyboard. 

# Exploratory data analysis
In this section exploratory data analysis is carried out to undersand word, bigram and trigram frequencies.

## Data set
Lets first figure out how big the data set is. 
```{r,echo=FALSE}
setwd("~/Documents/Git/Github/Word_Predictor/")
```
```{r,warning=FALSE,message=FALSE}
# Read number of lines,words in each file and measure its size
system("wc -lwc final/en_US/*.txt > temp.txt")
a <- read.table("temp.txt"); 
a <- data.frame("Filename" = a$V4, "Lines" = prettyNum(a$V1,big.mark = ","),
                "Words" = prettyNum(a$V2,big.mark = ","), "Size.MB" = round(a$V3/2^20,2))
library(knitr); kable(a,align="c")
```

There are about 4 million lines of text containing about 100 million words spread in files of size 550MB. Lets read a portion of this data into workspace and carry out analysis on that.


## Training, Validation and Test Data sets
Lets split 60% of data into training, 20% of data into validation and the remaning 20% of data into testing data sets. The idea is to grab a set of lines from each of the sources of data - blogs, news and twitter to for a particular type of data set - training/validation/testing sets.

```{r,warning=FALSE,message=FALSE}
# Process each input file. 
Nlines <- 100000
d.b <- readLines("final/en_US/en_US.blogs.txt",Nlines)
d.n <- readLines("final/en_US/en_US.news.txt",Nlines)
d.t <- readLines("final/en_US/en_US.twitter.txt",Nlines)

# Split data into training, validation and test sets
set.seed(100)
idx <- sample(seq(Nlines)); N1 <- round(0.6*Nlines); N2 <- round(0.8*Nlines)
train <- c(d.b[idx[1:N1]],          d.n[idx[1:N1]],          d.t[idx[1:N1]])
valid <- c(d.b[idx[(N1+1):N2]],     d.n[idx[(N1+1):N2]],     d.t[idx[(N1+1):N2]])
test  <- c(d.b[idx[(N2+1):Nlines]], d.n[idx[(N2+1):Nlines]], d.t[idx[(N2+1):Nlines]])
```
```{r,echo=FALSE}
rm(d.b,d.n,d.t)
x <- data.frame(c("Training Set","Validation Set","Testing Set", "Total"),
                prettyNum(3*c(N1,N2-N1,Nlines-N2,Nlines),big.mark = ","),
                round(c(N1,N2-N1,Nlines-N2,Nlines)/Nlines*100,1) )
names(x) <- c("Data Set", "Num lines", "Percentage lines")
library(knitr);kable(x,align = "c")
```

Lets now sample the training set and carry out analysis on uni-gram, bi-gram and tri-grams. Since modeling these N-grams is a computationally expensive step, I would like to take the approach of sampling the training set - obtain some metrics - infer the statistics of the  population (ie., entire training set). 

## Data cleansing and pre-processing
The following steps are carried out as part of **ngram_tokenize()** function while carrying out the analysis. This is included as part of **ngram_compute()** function. Refer to appendix section for implementation details.

1. Split the raw text sentences into words
2. Filter out punctuations. 
3. Convert all words to lowercase.
4. Filter out profane words. List of profane words are taken from [_**CMU website**_](http://www.cs.cmu.edu/~biglou/resources/bad-words.txt)
5. Filter out stop words. List of stop word taken from [_**MIT Paper**_](http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-list/english.stop)

```{r,echo=FALSE}
fileurl <- "http://www.cs.cmu.edu/~biglou/resources/bad-words.txt"
if(!file.exists("bad-words.txt")) { download.file(fileurl,destfile = "bad-words.txt",method="curl") }

fileurl <- "http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-list/english.stop"
if(!file.exists("stop-words.txt")) { download.file(fileurl,destfile = "stop-words.txt", method="curl") }

ngram_tokenize <- function(data) {
    for(i in 1:length(data)) {
        temp <- data[i]
        temp <- gsub("[.-]"," ", temp) # replace . - with space
        temp <- gsub("[[:punct:]]","",temp) # remove punctuation
        temp <- gsub("[0-9]","",temp) # remove numbers
        data[i] <- tolower(temp) # to lower case
    }
    data <- lapply(data, function(x) unlist(strsplit(x,split=" ")) ) # into words
    data <- lapply(data, function(x) grep("^[a-z]+$",x,value=TRUE) ) # select only english
    
    # Remove profane and remove stop words
    profanity <- readLines("bad-words.txt"); 
    stopWords <- readLines("stop-words.txt"); stopWords <- gsub("[[:punct:]]","",stopWords)
    remWords <- c(profanity[-1],stopWords)
    data <- lapply(data, function(x) { x[!(x %in% remWords)] })
}
```

## Uni-gram analysis
Lets look at the unigrams and their frequency of occurence in the training sets. The way to go about is to sample a small subset of data from the training set - compute the unigrams and their counts - repeat it for many such small subsets - average across all sample sets. To start with a sample size of **100** lines of text and **100** such samples are chosen. This equates to **10,000** lines of text.

```{r,echo=FALSE}
ngram_compute <- function(data,n,mat = 0,dictionary = character()) {
    data <- ngram_tokenize(data) # Tokenize the data
    # Create n-grams
    if(n>1) { data <- sapply(data,function(x) x[x %in% dictionary]) } # select words in dictionary
    if(n==2) { # Bigrams
        idx2 <- sapply(data, function(x) ifelse(length(x)>1,TRUE,FALSE)) # rows with atleast 2 words
        data <- lapply(data[idx2],function(x) { paste(x[1:(length(x)-1)],x[2:length(x)]) })
    }
    if(n==3) { # Trigrams
        idx3 <- sapply(data, function(x) ifelse(length(x)>2,TRUE,FALSE)) # rows with atleast 3 words
        data <- lapply(data[idx3],function(x) { paste(x[1:(length(x)-2)],x[2:(length(x)-1)],x[3:length(x)]) })
    }
    # Count unique n-grams
    if(mat==1) { # simulates term-document-matrix
        L <- length(data)
        unique_ngrams <- unique(unlist(data))
        ngram <- matrix(0,length(unique_ngrams),L); rownames(ngram) <- unique_ngrams; colnames(ngram) <- 1:L
        for(i in 1:L) { ns <- table(data[[i]]);  ngram[names(ns),i] <- ns }
    }else { # obtains ngram counts
        ngram <- sort(table(unlist(data)),decreasing = TRUE)
    }
    ngram
}
```

```{r}
M <- 1000
# Compute unigrams
unigrams <- ngram_compute(train[1:M],n=1,mat=1) # Refer appendix for function definition
```

**Top 10 unigrams and their average counts are**
```{r}
um <- unigrams %*% matrix(1,M,1) #row sums
names(um) <- rownames(unigrams); um <- sort(um, decreasing = TRUE)
```
```{r,warning=FALSE,message=FALSE, fig.align="center"}
um[1:10]
library(wordcloud)
wordcloud(names(um),um,max.words=300,scale = c(3,0.5),rot.per = 0.2, colors = brewer.pal(6,"Dark2"),random.order = FALSE)
```

**Statistical Inference**

Using the central limit theorem to infer the population statistics from sample statistics, the average counts from the samples estimates to the counts seen in population. Note the counts are not integers as they are averaged over all sample sets and hence they are estimates. 
```{r,fig.width=5,fig.height=6,fig.align="center"}
Ms <- 100; Mcol = floor(M/Ms)
mask_mat <- matrix(0,Mcol*Ms,Mcol); for(i in 1:Mcol) mask_mat[(((i-1)*Ms+1):(i*Ms)),i] <- 1
ums <- unigrams[names(um[1:10]),] %*% mask_mat; rownames(ums) <- names(um[1:10])
boxplot(t(ums[10:1,]),xlab = "frequency (Number of unigrams per 100 lines)", main = "Unigram frequencies",horizontal = T,las=1)
```

## Bi-gram and Tri-gram analysis
**Dictionary selection**
```{r}
sum(um); length(um)
round(c(sum(um[1:1100]), sum(um[1:5900]))/sum(um)*100,2)
```

In the test sample set considered with sample size of **100** lines and **100** such samples, there are **10,000** lines of text containig **17,684** words and **7,572** unique unigrams. Data shows that in a frequency sorted word dictionary, only **1100** unique words are neeed to cover about **50%** of all the word instance and **5,900** unique words are needed to cover about **90%** of all word instances. Lets consider **all** the unigrams to be part of the dictionary.

```{r}
dict <- names(um)
```

Lets look at the bigrams as well as trigrams and their frequency of occurence in the training sets. Here words that are not present in the dictionary are filtered out. 

```{r}
# Compute bigrams and trigrams
bigrams <- ngram_compute(train[1:M],n=2,mat=1,dict)
trigrams <- ngram_compute(train[1:M],n=3,mat=1,dict)

bm <- bigrams %*% matrix(1,ncol(bigrams),1); names(bm) <- rownames(bigrams); bm <- sort(bm,decreasing = TRUE)
tm <- trigrams %*% matrix(1,ncol(trigrams),1); names(tm) <- rownames(trigrams); tm <- sort(tm,decreasing = TRUE)
```

**The top 5 bigrams and trigrams and their total counts**
```{r,echo=FALSE,eval=TRUE,warning=FALSE,message=FALSE,fig.width=10,fig.height=6,fig.align="center"}
bm[1:5]
tm[1:5]
par(mfrow=c(1,2),mar=c(2,4,2,4))
wordcloud(names(bm),bm,max.words=150,scale=c(3,0.5),rot.per = 0.2, colors = brewer.pal(6,"Dark2"),random.order = FALSE)
title(main="Bigrams")
wordcloud(names(tm),tm,max.words=100,scale=c(3,0.1),rot.per = 0.3, colors = brewer.pal(6,"Dark2"),random.order = FALSE)
title(main="Trigrams")
```

# Prediction model
In this section various models would be computed and evaluated to select the best model. 

## Model Building 
Lets buid Unigram, Bigram and Trigram models on a larger data set. The following options are considered in model building.

1. **Smoothing** : Laplace smoothing of adding 1 to all the counts is used here to calculate probabilities for those missing unigrams. Out of vocaboulary words are marked as "UNK". 
2. **Stop words** : Stop words are removed from unigram, bigram and trigram models.
3. **Pruning** : Any ngrams with 2 counts or fewer are not considered into the model directly. They are indirectly included via smoothin option above.

```{r,echo=TRUE,eval=FALSE}
M <- 100000
u.count <- ngram_compute(train[1:M],n=1)   # Unigram model
dict <- names(u.count[u.count>2])    # Dictionary
b.count <- ngram_compute(train[1:M],n=2,dictionary = dict)   # Bigram model
t.count <- ngram_compute(train[1:M],n=3,dictionary = dict)   # Trigram model

save(u.count,b.count,t.count,file="Model_All.RData")
```
```{r,echo=FALSE}
load("Model_All.RData")
x <- data.frame(c("unigrams","bigrams","trigrams"),
                prettyNum(c(sum(u.count),sum(b.count),sum(t.count)),big.mark = ","),
                prettyNum(c(length(dict),length(b.count),length(t.count)),big.mark = ",") )
names(x) <- c("N-gram","Number of ngrams", "Unique no. of ngrams")
kable(x,align="c")
```

### Unigram, Bigram and Trigram probabilities
Compute unigram, bigram and trigram probability tables in this step. They are used to evaluate models and also predict the next word. 
```{r}
# Calculate unigram log likelihood
N <- sum(u.count) # Total number of tokens
V <- length(dict) # Number of words in the dictionary
u.model <- rbind(as.matrix(u.count[dict]),"UNK" = 0) + 1 # Laplacian smoothing
u.model <- cbind(u.model,log(u.model[,1]/(N+V)) ); colnames(u.model) <- c("count","MLE")

# Calculate bigram log likelihood
PQ <- names(b.count)
P <- sapply(strsplit(PQ, split = " "), function(x) x[1])
bm.1 <- as.matrix(b.count) + 1 # Laplacian smoothing
bm.1 <- cbind(bm.1,log(bm.1[,1]/u.model[P,1]) ) # "Word1" "Word2"
bm.2 <- as.matrix(cbind(rep(1,V),log(1/u.model[1:V,1]))) # "Word1" "UNK"
rownames(bm.2) <- paste(dict,"UNK")
bm.3 <- as.matrix(cbind(1,u.model["UNK",2])); rownames(bm.3) <- "UNK UNK" # "UNK" "UNK"
b.model <- rbind(bm.1,bm.2,bm.3); colnames(b.model) <- c("count","MLE")

# Calculate trigram log likelihood
PQR <- rownames(t.count)
PQ <- sapply(strsplit(PQR, split = " "), function(x) paste(x[1:2],collapse = " "))
t.model <- cbind(as.matrix(t.count+1), log(as.matrix(t.count+1)/b.model[PQ,1]) )
colnames(t.model) <- c("count","MLE")

# Save models to disk
save(u.count,b.count,t.count,u.model,b.model,t.model, file="Model_All.RData")
```

The number of tokens in the model **N = 7,112,600** and the number of words in the dictionary **V = 26,315**.

**Top 10 unigrams, bigrams and trigrams from the corpus are listed below**
```{r,echo=FALSE,eval=TRUE}
x <- data.frame(rownames(u.model[1:10,]),u.model[1:10,1],round(u.model[1:10,2],1),
                rownames(b.model[1:10,]),b.model[1:10,1],round(b.model[1:10,2],1),
                rownames(t.model[1:10,]),t.model[1:10,1],round(t.model[1:10,2],1),
                row.names = NULL)
names(x) = c("Unigrams","Count","MLE","Bigrams","Count","MLE","Trigrams","Count","MLE")
kable(x,align="c")
```

**A cursory check** : The log probability values calculated fall in between -inf to 0 that maps to probability values of 0 to 1 as expected.
```{r,fig.width=10}
summary(u.model[,2]); summary(b.model[,2]); summary(t.model[,2])
par(mfrow=c(1,3))
hist(u.model[,2],xlab="MLE", main= "Unigrams", col = "blue")
hist(b.model[,2],xlab="MLE", main="Bigrams", col = "green")
hist(t.model[,2],xlab="MLE", main="Trigrams",col = "salmon")
```

## Model comparison and selection
Lets compare the three models individually and as a group to see how best they work on the validation set. Perplexity would be calculated on the validation set and the one with the least perplexity wins.
```{r,echo=FALSE}
bigram_mle <- function(D,u.model,b.model,flag=0) {
    D[!(D %in% rownames(u.model))] <- "UNK"; 
    L <- length(D)
    if(flag == 0) { 
        Y <- D[1:(L-1)]
        Z <- D[2:L]
    } else { # special case for predicted word
        Z <- rownames(u.model); Z <- Z[1:(length(Z)-1)] # Map against dictionary  
        Y <- rep(D[L],length(Z))
    }
    YZ <- paste(Y,Z)
    id1 <- (YZ %in% rownames(b.model))
    id2 <- (!id1) & (Y != "UNK")
    id3 <- (!id1) & (Y == "UNK")
    a <- matrix(NA,length(YZ),1)
    if(sum(id1)>0) { a[id1] <- b.model[YZ[id1],2] }
    if(sum(id2)>0) { a[id2] <- log(1/u.model[Y[id2],1]) }
    if(sum(id3)>0) { a[id3] <- u.model["UNK",2] }
    rownames(a) <- YZ; colnames(a) <- "MLE"
    a
}
trigram_mle <- function(D,u.model,b.model,t.model,flag = 0) {
    D[!(D %in% rownames(u.model))] <- "UNK"; L <- length(D)
    if(flag == 0) {
        P <- D[1:(L-2)]; Q <- D[2:(L-1)]; R <- D[3:L]
    }else { # special case for predicted word
        R <- rownames(u.model); R <- R[1:(length(R)-1)] # Map against dictionary
        P <- rep(D[L-1],length(R)) 
        Q <- rep(D[L],length(R))
    }
    PQR <- paste(P,Q,R)
    id1 <- (PQR %in% rownames(t.model))
    PQ <- paste(P,Q)
    id2 <- (!id1) & (PQ %in% rownames(b.model))
    id3 <- (!id1) & (!(PQ %in% rownames(b.model))) & (P != "UNK")
    id4 <- (!id1) & (!(PQ %in% rownames(b.model))) & (P == "UNK")
    
    a <- matrix(NA,length(PQR),1)
    if(sum(id1)>0) { a[id1] <- t.model[PQR[id1],2] }
    if(sum(id2)>0) { a[id2] <- log(1/b.model[PQ[id2],1]) }
    if(sum(id3)>0) { a[id3] <- log(1/u.model[P[id3],1]) }
    if(sum(id4)>0) { a[id4] <- u.model["UNK",2] }
    
    rownames(a) <- PQR; colnames(a) <- "MLE"
    a
}
ngram_perplexity <- function(data,u.model,b.model,t.model,n) {
    D <- ngram_tokenize(data); D <- unlist(D); 
    if(n==1) { # Unigram
        D[!(D %in% rownames(u.model))] <- "UNK"
        a <- u.model[D,2]
        n_perp <- exp(-sum(a)/length(a))
    }
    if(n==2) { # Bigram model
        a <- bigram_mle(D,u.model,b.model)
        n_perp <- exp(-sum(a)/length(a))
    }
    if(n==3) { # Trigram model
        a <- trigram_mle(D,u.model,b.model,t.model)
        n_perp <- exp(-sum(a)/length(a))
    }
    n_perp
}
```

```{r}
u_perp <- ngram_perplexity(valid[1:10000],u.model,b.model,t.model,1) # Unigram model perplexity
b_perp <- ngram_perplexity(valid[1:10000],u.model,b.model,t.model,2) # Bigram model perplexity
t_perp <- ngram_perplexity(valid[1:10000],u.model,b.model,t.model,3) # Trigram model perplexity
```
```{r,echo=FALSE}
x <- data.frame(Model = c("Unigram","Bigram","Trigram"),
                Num = prettyNum(c(nrow(u.model), nrow(b.model), nrow(t.model)),big.mark = ","),
                Perp = round(c(u_perp, b_perp, t_perp),2))
names(x) <- c("Model Type", "Number of unique n-grams", "Perplexity (validation set)")
kable(x,align="c")
```

From the perplexity numbers, the model of choice would be **bigram model - with smoothing** as it has the least perplexity on validation data set.

## Example prediction
Lets run through example test cases
```{r,echo=FALSE}
ngram_predict <- function(data) {
    #load("Model.RData")
    
    D <- ngram_tokenize(data) # Tokenize
    D <- unlist(D); D[!(D %in% rownames(u.model))] <- "UNK" # mark the ones not in dictionary as UNK
    L <- length(D)
    
    if(L==0) { P <- "UNK"; Q <- "UNK"}
    if(L==1) { P <- "UNK"; Q <- D[L] }
    if(L>1) { P <- D[L-1]; Q <- D[L] }
    R <- rownames(u.model); R <- R[-length(R)] # Map against dictionary
    
    PQR <- paste(P,Q,R)
    id1 <- (PQR %in% rownames(t.model))
    PQ <- paste(P,Q)
    QR <- paste(Q,R)
    id2 <- (!id1) & (PQ %in% rownames(b.model))    &   (QR %in% rownames(b.model))
    id3 <- (!id1) & (PQ %in% rownames(b.model))    & (!(QR %in% rownames(b.model)))
    id4 <- (!id1) & (!(PQ %in% rownames(b.model))) &   (QR %in% rownames(b.model))
    id5 <- (!id1) & (!(PQ %in% rownames(b.model))) & (!(QR %in% rownames(b.model)))
    
    a <- matrix(NA,length(PQR),1)
    if(sum(id1)>0) { a[id1] <- t.model[PQR[id1],2] }
    if(sum(id2)>0) { a[id2] <- b.model[PQ,2] + b.model[QR[id2],2] + log(2*0.4) }
    if(sum(id3)>0) { a[id3] <- b.model[PQ,2] + u.model[Q,2] + u.model[R[id3],2] + log(3*0.4) }
    if(sum(id4)>0) { a[id4] <- u.model[P,2] + u.model[Q,2] + b.model[QR[id4],2] + log(3*0.4) }
    if(sum(id5)>0) { a[id5] <- u.model[P,2] + u.model[Q,2] + u.model[R[id5],2] + log(4*0.4) }
    
    pred_word <- rownames(u.model)[which.max(a)]
}
```

```{r,eval=TRUE}
df <- data.frame()
for(i in 1:100) {
    data <- unlist(strsplit(test[i],split = " "))
    newdata <- paste(data[1:(length(data)-1)],collapse = " ")
    truth <- data[(length(data)-10):length(data)]
    pred <- ngram_predict(newdata)
    df <- rbind(df, data.frame(truth = truth, pred = pred))
}
df
```

The idea here is to use limited vocabulary unigram, bigram and trigram dictionaries and see what best we can do.

# Appendix - Code

### ngram_tokenize()
Function that tokenizes lines of text into words
```{r,eval=FALSE}
ngram_tokenize <- function(data) {
    for(i in 1:length(data)) {
        temp <- data[i]
        temp <- gsub("[.-]"," ", temp) # replace . - with space
        temp <- gsub("[[:punct:]]","",temp) # remove punctuation
        temp <- gsub("[0-9]","",temp) # remove numbers
        data[i] <- tolower(temp) # to lower case
    }
    data <- lapply(data, function(x) unlist(strsplit(x,split=" ")) ) # into words
    data <- lapply(data, function(x) grep("^[a-z]+$",x,value=TRUE) ) # select only english
    
    # Remove profane and remove stop words
    profanity <- readLines("bad-words.txt"); 
    stopWords <- readLines("stop-words.txt"); stopWords <- gsub("[[:punct:]]","",stopWords)
    remWords <- c(profanity[-1],stopWords)
    data <- lapply(data, function(x) { x[!(x %in% remWords)] })
}
```

### ngram_compute()
Function to compute counts of ngrams. This is equivalent to NGramTokenizer() and TermDocumentMatrix() combined.
```{r, eval = FALSE}
ngram_compute <- function(data,n,mat = 0,dictionary = character()) {
    data <- ngram_tokenize(data) # Tokenize the data
    # Create n-grams
    if(n>1) { data <- sapply(data,function(x) x[x %in% dictionary]) } # select words in dictionary
    if(n==2) { # Bigrams
        idx2 <- sapply(data, function(x) ifelse(length(x)>1,TRUE,FALSE)) # rows with atleast 2 words
        data <- lapply(data[idx2],function(x) { paste(x[1:(length(x)-1)],x[2:length(x)]) })
    }
    if(n==3) { # Trigrams
        idx3 <- sapply(data, function(x) ifelse(length(x)>2,TRUE,FALSE)) # rows with atleast 3 words
        data <- lapply(data[idx3],function(x) { paste(x[1:(length(x)-2)],x[2:(length(x)-1)],x[3:length(x)]) })
    }
    # Count unique n-grams
    if(mat==1) { # simulates term-document-matrix
        L <- length(data)
        unique_ngrams <- unique(unlist(data))
        ngram <- matrix(0,length(unique_ngrams),L); rownames(ngram) <- unique_ngrams; colnames(ngram) <- 1:L
        for(i in 1:L) { ns <- table(data[[i]]);  ngram[names(ns),i] <- ns }
    }else { # obtains ngram counts
        ngram <- sort(table(unlist(data)),decreasing = TRUE)
    }
    ngram
}
```

### bigram_mle()
Function to compute bigram MLE of a test data set. It can also compute MLE against a dictionary which can be used to predict the next word.
```{r,eval=FALSE}
bigram_mle <- function(D,u.model,b.model,flag=0) {
    D[!(D %in% rownames(u.model))] <- "UNK"; 
    L <- length(D)
    if(flag == 0) { 
        Y <- D[1:(L-1)]
        Z <- D[2:L]
    } else { # special case for predicted word
        Z <- rownames(u.model); Z <- Z[1:(length(Z)-1)] # Map against dictionary  
        Y <- rep(D[L],length(Z))
    }
    YZ <- paste(Y,Z)
    id1 <- (YZ %in% rownames(b.model))
    id2 <- (!id1) & (Y != "UNK")
    id3 <- (!id1) & (Y == "UNK")
    a <- matrix(NA,length(YZ),1)
    if(sum(id1)>0) { a[id1] <- b.model[YZ[id1],2] }
    if(sum(id2)>0) { a[id2] <- log(1/u.model[Y[id2],1]) }
    if(sum(id3)>0) { a[id3] <- u.model["UNK",2] }
    rownames(a) <- YZ; colnames(a) <- "MLE"
    a
}
```

### trigram_mle()
Function to compute trigram MLE of a test data set. It can also compute MLE against a dictionary which can be used to predict the next word.
```{r,eval=FALSE}
trigram_mle <- function(D,u.model,b.model,t.model,flag = 0) {
    D[!(D %in% rownames(u.model))] <- "UNK"; L <- length(D)
    if(flag == 0) {
        P <- D[1:(L-2)]; Q <- D[2:(L-1)]; R <- D[3:L]
    }else { # special case for predicted word
        R <- rownames(u.model); R <- R[1:(length(R)-1)] # Map against dictionary
        P <- rep(D[L-1],length(R)) 
        Q <- rep(D[L],length(R))
    }
    PQR <- paste(P,Q,R)
    id1 <- (PQR %in% rownames(t.model))
    PQ <- paste(P,Q)
    id2 <- (!id1) & (PQ %in% rownames(b.model))
    id3 <- (!id1) & (!(PQ %in% rownames(b.model))) & (P != "UNK")
    id4 <- (!id1) & (!(PQ %in% rownames(b.model))) & (P == "UNK")
    
    a <- matrix(NA,length(PQR),1)
    if(sum(id1)>0) { a[id1] <- t.model[PQR[id1],2] }
    if(sum(id2)>0) { a[id2] <- log(1/b.model[PQ[id2],1]) }
    if(sum(id3)>0) { a[id3] <- log(1/u.model[P[id3],1]) }
    if(sum(id4)>0) { a[id4] <- u.model["UNK",2] }
    
    rownames(a) <- PQR; colnames(a) <- "MLE"
    a
}
```

### ngram_perplexity()
Function to compute perplexity of a test data set. Loops are unrolled for fast performance
```{r,eval=FALSE}
ngram_perplexity <- function(data,u.model,b.model,t.model,n) {
    D <- ngram_tokenize(data); D <- unlist(D); 
    if(n==1) { # Unigram
        D[!(D %in% rownames(u.model))] <- "UNK"
        a <- u.model[D,2]
        n_perp <- exp(-sum(a)/length(a))
    }
    if(n==2) { # Bigram model
        a <- bigram_mle(D,u.model,b.model)
        n_perp <- exp(-sum(a)/length(a))
    }
    if(n==3) { # Trigram model
        a <- trigram_mle(D,u.model,b.model,t.model)
        n_perp <- exp(-sum(a)/length(a))
    }
    n_perp
}
```

### ngram_predict()
Function to predict the next word given a sequence of words
```{r,eval=FALSE}
ngram_predict <- function(data) {
    #load("Model.RData")
    
    D <- ngram_tokenize(data) # Tokenize
    D <- unlist(D); D[!(D %in% rownames(u.model))] <- "UNK" # mark the ones not in dictionary as UNK
    L <- length(D)
    
    if(L==0) { P <- "UNK"; Q <- "UNK"}
    if(L==1) { P <- "UNK"; Q <- D[L] }
    if(L>1) { P <- D[L-1]; Q <- D[L] }
    R <- rownames(u.model); R <- R[-length(R)] # Map against dictionary
    
    PQR <- paste(P,Q,R)
    id1 <- (PQR %in% rownames(t.model))
    PQ <- paste(P,Q)
    QR <- paste(Q,R)
    id2 <- (!id1) & (PQ %in% rownames(b.model))    &   (QR %in% rownames(b.model))
    id3 <- (!id1) & (PQ %in% rownames(b.model))    & (!(QR %in% rownames(b.model)))
    id4 <- (!id1) & (!(PQ %in% rownames(b.model))) &   (QR %in% rownames(b.model))
    id5 <- (!id1) & (!(PQ %in% rownames(b.model))) & (!(QR %in% rownames(b.model)))
    
    a <- matrix(NA,length(PQR),1)
    if(sum(id1)>0) { a[id1] <- t.model[PQR[id1],2] }
    if(sum(id2)>0) { a[id2] <- b.model[PQ,2] + b.model[QR[id2],2] + log(2*0.4) }
    if(sum(id3)>0) { a[id3] <- b.model[PQ,2] + u.model[Q,2] + u.model[R[id3],2] + log(3*0.4) }
    if(sum(id4)>0) { a[id4] <- u.model[P,2] + u.model[Q,2] + b.model[QR[id4],2] + log(3*0.4) }
    if(sum(id5)>0) { a[id5] <- u.model[P,2] + u.model[Q,2] + u.model[R[id5],2] + log(4*0.4) }
    
    pred_word <- rownames(u.model)[which.max(a)]
}
```
